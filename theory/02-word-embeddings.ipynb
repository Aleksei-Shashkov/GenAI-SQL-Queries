{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding\n",
    "\n",
    "## Why Embedding?\n",
    "As we know, machines can't handle text, it can only handle numbers. But how to convert a word to numbers?\n",
    "\n",
    "The most naive approach would be to take a list of all the words in your text and attribute a number to all of them. It will work but you can imagine that some problems will appear:\n",
    "* How do you handle unknown words? \n",
    "* If your text contains `doctor`, `nurse`, and `candy`. `doctor` and `nurse` have a strong similarity but `candy` doesn't. How can we make the machine understand that? With our naive technique, `doctor` could have the number `5` associated to it and nurse the number `98767`.\n",
    "\n",
    "Of course, a lot of people already spent some time with those problems. the solution that came out of it is \"Embedding\". \n",
    "\n",
    "## What is embeddings?\n",
    "\n",
    "An embedding is a **VECTOR** which represents a word or a document.\n",
    "\n",
    "A vector will be attributed to each token. Each vector will contain multiple dimensions (usually tens or hundreds of dimensions).\n",
    "\n",
    "```\n",
    "[...] associate with each word in the vocabulary a distributed word feature vector [...] The feature vector represents different aspects of the word: each word is associated with a point in a vector space. The number of features [...] is much smaller than the size of the vocabulary.\n",
    "```\n",
    "- [A Neural Probabilistic Language Model](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf), 2003.\n",
    "\n",
    "Long story short, embeddings convert words into vectors in a way that allows the machine to understand the similarity betweens them.\n",
    "\n",
    "Each embedding library has it's own way of classifying words, it will regroup words into big categories. Each word will get a score for each category.\n",
    "\n",
    "To take a simple example the word `mother` could be classified like that:\n",
    "\n",
    "|        | female | family | human | animal|\n",
    "|--------|--------|---------|-------|-------|\n",
    "| mother | 0.9    | 0.9.    | 0.7   | 0.1   |\n",
    "\n",
    "**Explanations:** Mother has a strong similarity with female, family and human but it has a low similarity with animal.\n",
    "\n",
    "**Disclaimer:** Those numbers and categories are totally arbitrary and are only here to show an example.\n",
    "\n",
    "Here is another example with more complete datas:\n",
    "\n",
    "![embedding](https://miro.medium.com/max/2598/1*sAJdxEsDjsPMioHyzlN3_A.png)\n",
    "\n",
    "## Should I do it by hand?\n",
    "\n",
    "You could, but if some people already did the job for you and spent a lot of time to optimize it, why not use it?\n",
    "\n",
    "## What to use?\n",
    "\n",
    "There are a lot of libraries out there for embeddings. Which one is the best? Once again, *it depends*. The results will change depending on the text you are using, the information you want to extract, the model you use,...\n",
    "\n",
    "Choosing the \"best\" embedding model will be part of the hyper-optimization that you can do at the end of a project.\n",
    "\n",
    "If you want understand embeddings more in depth, [follow this link](http://jalammar.github.io/illustrated-word2vec/) or watch this [video](https://www.youtube.com/watch?v=gQddtTdmG_8).\n",
    "\n",
    "Here are some of the best libraries:\n",
    "\n",
    "* [Gensim](https://pypi.org/project/gensim/)\n",
    "* [Word2Vec](https://www.tensorflow.org/tutorials/text/word2vec)\n",
    "\n",
    "This next bit of code loads a model for practice. If you get an error, it may be due to the Python version (3.12.3 works), make sure you create a venv for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import os\\nimport gensim.downloader as api\\nfrom gensim.models import KeyedVectors\\nimport math\\nimport numpy as np\\n\\n# Path where you want to store/load the model\\nmodel_path = \"glove-wiki-gigaword-300.kv\"\\n\\n# Load model from disk if exists, else download and save it\\nif os.path.exists(model_path):\\n    print(\"Loading model from local file...\")\\n    model = KeyedVectors.load(\"data/\"+model_path)\\nelse:\\n    print(\"Downloading model...\")\\n    model = api.load(model_path[:-3])\\n    model.save(\"data/\"+model_path)\\n    print(\"Model downloaded and saved.\")'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import os\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# Path where you want to store/load the model\n",
    "model_path = \"glove-wiki-gigaword-300.kv\"\n",
    "\n",
    "# Load model from disk if exists, else download and save it\n",
    "if os.path.exists(model_path):\n",
    "    print(\"Loading model from local file...\")\n",
    "    model = KeyedVectors.load(\"data/\"+model_path)\n",
    "else:\n",
    "    print(\"Downloading model...\")\n",
    "    model = api.load(model_path[:-3])\n",
    "    model.save(\"data/\"+model_path)\n",
    "    print(\"Model downloaded and saved.\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "import math\n",
    "import numpy as np\n",
    "model_path = \"glove-wiki-gigaword-300.kv\"\n",
    "model = KeyedVectors.load(\"data/\"+model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice time!\n",
    "\n",
    "Enough reading, let's practice a bit. On this sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I love learning\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do the word vectors look like? What is their size? What is their [magnitude](https://numpy.org/doc/2.1/reference/generated/numpy.linalg.norm.html)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'love', 'learning']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#you may get the vectors from using the model like a dictionary\n",
    "# разбивка предложения на слова\n",
    "tokens = sentence.lower().split()\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': array([-1.3292e-01,  1.6985e-01, -1.4360e-01, -8.8722e-02,  7.9510e-02,\n",
       "        -1.4212e-01, -2.4209e-02, -2.6291e-01, -7.4814e-02, -2.3600e+00,\n",
       "         3.4830e-01, -9.1722e-02, -5.3906e-02,  3.0418e-01, -1.3286e-01,\n",
       "         5.0341e-03, -1.5056e-01,  2.3562e-03,  6.8321e-02,  3.4246e-01,\n",
       "         3.9891e-01,  5.8813e-01,  6.0618e-02, -1.9871e-01, -4.0465e-01,\n",
       "        -1.0706e-01, -5.9312e-03, -6.4842e-01,  1.9080e-01, -1.7630e-01,\n",
       "         9.2407e-02,  3.8685e-01, -3.1085e-01, -3.2574e-01, -1.6823e+00,\n",
       "         2.5336e-01, -2.4647e-01, -1.0874e-01,  7.6402e-03,  3.3880e-01,\n",
       "        -5.9736e-02, -8.5940e-01, -8.0964e-02, -2.2981e-01,  1.7709e-01,\n",
       "         8.2094e-02,  7.4416e-01,  3.6873e-01,  1.3740e-01,  2.9408e-01,\n",
       "         1.0647e-01, -1.3246e-01,  1.2134e-01, -1.4273e-01, -5.3270e-01,\n",
       "         6.4936e-01,  4.9657e-01,  3.0029e-01,  6.7226e-01,  1.8005e-01,\n",
       "         8.8050e-01,  3.8144e-02, -8.7140e-02,  7.6400e-01, -1.2107e-01,\n",
       "        -4.2809e-01, -1.2588e-01,  8.8377e-04,  1.0596e-01, -3.0802e-01,\n",
       "         2.2887e-01, -2.5468e-01, -3.6484e-01,  7.4524e-01, -1.5217e-01,\n",
       "        -5.5619e-02,  1.2049e-01,  3.9876e-01, -2.1991e-01, -1.8444e-01,\n",
       "        -9.0398e-02,  1.4077e-01,  1.2865e+00, -4.0910e-01,  2.7999e-01,\n",
       "         4.2089e-01, -7.0627e-01,  7.9004e-01,  1.3804e-01,  3.0273e-01,\n",
       "        -1.8534e-01,  1.8121e-01, -7.1732e-01, -5.4259e-01, -2.2513e-03,\n",
       "         3.0463e-01, -4.9588e-01, -1.7462e-01,  2.1335e-02, -2.1341e-01,\n",
       "        -8.2391e-02, -3.7676e-01, -2.8001e-01, -2.4238e-01, -3.5249e-01,\n",
       "        -2.4116e-01,  2.9776e-01,  3.7027e-01, -1.7822e-01,  4.7537e-01,\n",
       "        -7.3484e-02, -2.9747e-01, -3.6304e-02, -8.4941e-01,  4.5171e-02,\n",
       "         2.8779e-01, -1.7543e-01, -2.7925e-01, -9.9789e-02, -1.3493e-01,\n",
       "        -7.8184e-02, -3.5548e-01,  2.1692e-01,  2.5895e-01,  2.6050e-01,\n",
       "        -4.3193e-01,  1.2076e-01,  2.8405e-01,  1.5691e-01,  2.1351e-01,\n",
       "         5.7355e-01, -4.7598e-01,  4.5758e-02,  6.5907e-02, -2.4157e-01,\n",
       "         1.5724e-01, -3.9697e-01, -8.7238e-02,  1.1132e-02,  4.6365e-01,\n",
       "        -4.3082e-02,  5.6318e-01,  5.2448e-01,  3.8723e-01, -4.4893e-01,\n",
       "        -3.6617e-03, -1.4007e-01,  2.9963e-02, -1.4168e-01,  5.1547e-02,\n",
       "        -7.6266e-01, -8.0719e-02, -1.0827e-01,  2.0345e-02, -1.4594e-01,\n",
       "         1.3781e-01,  3.5217e-01, -1.9425e-01,  4.7376e-01, -2.7950e-01,\n",
       "        -8.3159e-02, -6.1386e-01, -3.4310e-01, -9.9814e-02, -2.4054e-02,\n",
       "         2.0529e-01, -2.2244e-02,  5.9102e-01,  1.7002e-01,  5.2526e-02,\n",
       "        -1.9144e-01,  1.2114e-01, -5.0691e-01, -5.1987e-01,  5.4760e-02,\n",
       "         1.5022e-01,  9.2612e-02,  3.4548e-01,  3.5603e-01,  2.3423e-01,\n",
       "         1.3452e-01,  1.5486e-02,  1.7266e-01, -1.8454e-01, -9.8682e-02,\n",
       "        -2.1803e-01, -1.5786e-01,  7.0495e-01,  3.6798e-01,  2.6764e-01,\n",
       "        -1.4352e-01, -5.4373e-02,  4.3326e-01, -1.6332e-01,  2.1658e-02,\n",
       "        -5.3408e-01, -2.5548e-02,  4.1534e-01, -1.6234e-01, -4.0243e-01,\n",
       "         2.0005e+00, -1.3084e-01,  1.1824e-01,  1.8055e-01,  3.1811e-01,\n",
       "         1.5508e-01,  3.6686e-02,  3.0747e-01,  1.9520e-01, -4.4309e-02,\n",
       "        -6.5875e-01,  1.8604e-01, -1.6245e-01,  4.0257e-02, -5.3955e-01,\n",
       "        -1.2123e-02, -3.0579e-03,  1.7945e-02,  1.4503e-01, -3.5640e-01,\n",
       "         4.3888e-01, -1.1453e-01, -1.5161e-01,  2.4664e-01,  4.3432e-01,\n",
       "         3.1283e-01,  4.2919e-01, -1.2493e-01, -1.1767e-01, -2.2056e-04,\n",
       "        -3.5881e-01, -6.7778e-02,  2.1444e-01, -7.9171e-01, -1.1005e-01,\n",
       "         3.0940e-01, -3.0179e-03,  1.3354e-01, -2.0436e-01, -1.6333e-01,\n",
       "        -4.8577e-02, -5.1513e-02,  3.1318e-02,  3.6954e-01, -4.7448e-02,\n",
       "        -7.6416e-02, -7.8986e-05, -7.2528e-02, -8.4875e-02, -1.4013e-01,\n",
       "         2.0147e-01, -2.7516e-01, -4.8958e-02,  3.1184e-01,  7.1849e-01,\n",
       "         3.0594e-01,  4.7019e-01, -2.8368e-01,  1.0073e-01, -4.1866e-01,\n",
       "         1.7655e-01, -1.2393e-01, -4.7247e-02, -8.9221e-02,  6.8069e-02,\n",
       "         1.4806e-02, -2.8357e-01, -2.5358e-01,  1.3204e-01,  6.9809e-01,\n",
       "        -8.4020e-02, -4.2223e-01,  5.5961e-01,  3.9954e-02,  5.8968e-02,\n",
       "        -1.9102e-01, -2.2069e+00,  8.1526e-02,  2.5289e-01, -2.2367e-01,\n",
       "        -3.0142e-01, -2.8467e-01, -8.7759e-02, -3.9672e-01, -2.4318e-01,\n",
       "         1.4144e-01, -1.2311e-01,  6.3953e-03, -3.4761e-01, -4.6531e-01,\n",
       "        -1.0615e-03, -3.9291e-01, -1.7780e-01, -1.2175e-01,  2.4335e-01,\n",
       "        -7.7835e-01,  3.9598e-01, -2.3778e-01,  1.4766e-01,  6.2902e-01],\n",
       "       dtype=float32),\n",
       " 'love': array([-4.5205e-01, -3.3122e-01, -6.3607e-02,  2.8325e-02, -2.1372e-01,\n",
       "         1.6839e-01, -1.7186e-02,  4.7309e-02, -5.2355e-02, -9.8706e-01,\n",
       "         5.3762e-01, -2.6893e-01, -5.4294e-01,  7.2487e-02,  6.6193e-02,\n",
       "        -2.1814e-01, -1.2113e-01, -2.8832e-01,  4.8161e-01,  6.9185e-01,\n",
       "        -2.0022e-01,  1.0082e+00, -1.1865e-01,  5.8710e-01,  1.8482e-01,\n",
       "         4.5799e-02, -1.7836e-02, -3.3952e-01,  2.9314e-01, -1.9951e-01,\n",
       "        -1.8930e-01,  4.3267e-01, -6.3181e-01, -2.9510e-01, -1.0547e+00,\n",
       "         1.8231e-01, -4.5040e-01, -2.7800e-01, -1.4021e-01,  3.6785e-02,\n",
       "         2.6487e-01, -6.6712e-01, -1.5204e-01, -3.5001e-01,  4.0864e-01,\n",
       "        -7.3615e-02,  6.7630e-01,  1.8274e-01, -4.1660e-02,  1.5014e-02,\n",
       "         2.5216e-01, -1.0109e-01,  3.1915e-02, -1.1298e-01, -4.0147e-01,\n",
       "         1.7274e-01,  1.8497e-03,  2.4456e-01,  6.8777e-01, -2.7019e-01,\n",
       "         8.0728e-01, -5.8296e-02,  4.0550e-01,  3.9893e-01, -9.1688e-02,\n",
       "        -5.2080e-01,  2.4570e-01,  6.3001e-02,  2.1421e-01,  3.3197e-01,\n",
       "        -3.4299e-01, -4.8735e-01,  2.2264e-02,  2.7862e-01,  2.3881e-01,\n",
       "         9.7794e-02,  3.8023e-01, -3.7744e-02, -4.1966e-01, -1.9145e-01,\n",
       "        -9.5830e-02,  2.6871e-01,  5.2876e-01, -2.6870e-01, -3.4450e-01,\n",
       "         2.5413e-01,  1.3606e-01,  3.6528e-01,  8.1960e-02, -5.2224e-01,\n",
       "         3.4159e-02,  1.7019e-01, -1.0520e-01, -8.0873e-01, -1.8095e-01,\n",
       "        -6.0823e-02,  2.6014e-01, -7.8717e-02, -1.5161e-01, -6.8438e-01,\n",
       "         2.1809e-01,  4.7330e-02, -1.2147e-01, -3.3892e-01, -1.8629e-02,\n",
       "         2.4546e-01,  2.7374e-01, -2.7045e-01, -8.7233e-02, -4.7871e-01,\n",
       "         1.9630e-01,  2.3124e-02,  1.9453e-01, -4.6860e-01,  4.4499e-01,\n",
       "         1.8360e-01, -1.6480e-01,  3.8598e-01,  3.6070e-01, -3.1108e-01,\n",
       "        -3.4627e-01,  1.9004e-01, -9.8683e-02,  3.3821e-01, -1.3978e-01,\n",
       "        -7.2723e-01, -1.0660e-01,  1.9208e-03, -3.3093e-01,  3.5117e-01,\n",
       "         1.5491e-01,  1.7150e-01,  2.8933e-01, -4.9213e-02, -5.0567e-01,\n",
       "        -2.3513e-01, -2.8005e-01, -2.8487e-01, -2.4393e-01, -4.0838e-02,\n",
       "        -2.7223e-01,  9.4564e-02,  1.7332e-01,  1.8691e-01, -1.3585e-01,\n",
       "        -4.6439e-01, -4.5677e-01,  9.7890e-02,  3.9113e-02,  1.7901e-01,\n",
       "        -4.5629e-01,  4.7520e-01, -2.0388e-01,  2.4123e-01,  6.7551e-01,\n",
       "         6.2105e-02, -1.8940e-01,  1.5054e-01,  1.5875e-01, -2.3365e-01,\n",
       "         3.7429e-01, -2.3463e-01,  4.0692e-01,  1.4035e-01, -4.2971e-01,\n",
       "         5.6713e-01, -2.6706e-01, -6.8028e-02, -5.1264e-01, -3.0945e-01,\n",
       "        -3.9065e-01, -2.7004e-01, -1.1802e+00,  6.6480e-01, -2.4366e-01,\n",
       "         3.9183e-01, -2.8970e-01, -1.8839e-01, -4.9282e-01,  1.4538e-01,\n",
       "         2.4467e-01, -4.4340e-03,  2.3265e-01,  7.4544e-02, -3.0006e-01,\n",
       "        -3.0272e-01, -1.2394e-01,  3.3473e-01,  3.3880e-01,  9.7445e-02,\n",
       "        -3.3773e-01, -5.4316e-01, -4.7514e-01, -1.5696e-01, -9.3516e-01,\n",
       "        -8.7034e-02, -2.6743e-01,  6.4641e-04,  3.1940e-01, -6.2562e-03,\n",
       "         1.5854e+00,  1.2484e-01,  4.8481e-01,  7.5394e-02,  1.8963e-01,\n",
       "        -1.0226e-01,  4.7413e-01,  7.0402e-01, -6.4418e-02, -1.0181e-02,\n",
       "        -6.8619e-01,  1.5335e-02,  4.7851e-02,  3.7650e-01,  1.0487e-01,\n",
       "         2.2974e-01,  4.5352e-01,  3.1482e-01,  5.8880e-02,  6.7096e-02,\n",
       "         1.5679e-01,  1.3099e-01,  3.4581e-02, -7.3896e-02, -3.6433e-01,\n",
       "        -1.8847e-01,  4.1556e-02, -2.0124e-01, -7.3918e-02,  3.8752e-01,\n",
       "         1.6759e-01, -4.0627e-01, -1.3220e-02, -7.1793e-01, -2.2904e-01,\n",
       "         2.0474e-01, -1.3648e-01,  3.7779e-01, -4.0003e-01, -5.0106e-02,\n",
       "        -3.7799e-01,  7.2109e-02,  2.3057e-02,  2.3879e-01, -2.2423e-01,\n",
       "        -8.4834e-02, -7.0726e-01, -1.6182e-01,  2.6373e-01,  1.2226e-01,\n",
       "         8.0282e-02,  7.5963e-02, -3.4695e-01,  3.7703e-01,  3.6612e-01,\n",
       "        -9.8416e-02,  4.7400e-01, -2.5085e-01,  1.8641e-01, -2.5993e-01,\n",
       "         3.9035e-02, -6.1394e-01,  1.4721e-01, -6.7619e-01, -1.2489e-01,\n",
       "         3.8031e-01, -2.8061e-01, -3.9680e-01,  2.4165e-02,  9.8706e-02,\n",
       "         2.4838e-01, -4.5881e-01,  2.0310e-01, -4.3515e-01, -7.7804e-03,\n",
       "        -2.4642e-02, -1.3204e+00, -4.1078e-01,  2.8145e-01,  2.8188e-02,\n",
       "        -2.0470e-01,  1.6037e-01, -1.6696e-01,  1.3761e-01, -3.6714e-01,\n",
       "         1.3801e-01, -2.0688e-01,  3.8274e-01,  2.1289e-01, -7.5001e-02,\n",
       "        -5.0367e-01, -2.9295e-02, -2.1254e-02, -2.4253e-01,  3.3530e-01,\n",
       "        -3.5534e-01,  2.5358e-01,  3.8906e-02,  2.4314e-01, -2.8696e-02],\n",
       "       dtype=float32),\n",
       " 'learning': array([-3.8732e-01, -3.2467e-01,  2.2053e-01, -1.2059e-01, -1.4239e-01,\n",
       "         1.7644e-03,  4.7267e-02, -1.0137e-01,  1.6160e-02, -1.4371e+00,\n",
       "         2.1567e-01, -1.7872e-01, -3.7325e-01,  2.2951e-01, -1.3685e-01,\n",
       "        -2.3411e-01,  3.1486e-02, -1.1184e-01,  2.8801e-01,  5.2982e-01,\n",
       "         1.6756e-01, -2.9087e-01, -4.2971e-01,  1.5370e-01,  2.7230e-01,\n",
       "         8.1404e-02, -1.0030e-01, -3.4384e-02,  2.1093e-01,  2.6351e-01,\n",
       "        -9.9556e-02,  6.4533e-02,  9.1433e-02,  2.6835e-02, -7.3282e-01,\n",
       "         5.8963e-01,  2.9665e-01, -1.5700e-01,  4.1203e-02, -3.0608e-01,\n",
       "         2.0008e-01, -1.4383e-01,  1.6882e-02, -5.1156e-01,  3.5024e-02,\n",
       "         1.4701e-01,  4.4045e-01,  7.8400e-01,  4.2995e-01, -4.0588e-01,\n",
       "         1.7257e-01, -3.1089e-01,  3.0088e-01, -4.6342e-01, -4.6932e-01,\n",
       "         1.1143e-01,  4.2088e-01, -3.5451e-01,  2.4617e-01,  4.1219e-01,\n",
       "         6.6917e-01,  2.1395e-01,  4.0693e-01,  1.0106e-01,  4.3594e-03,\n",
       "         3.2127e-01,  1.8859e-01, -4.2245e-03, -5.0203e-01,  4.2274e-01,\n",
       "        -4.8717e-01, -2.8037e-01, -3.0031e-01,  2.0644e-01, -4.5693e-03,\n",
       "        -4.8045e-01, -8.5435e-02,  1.7425e-01,  1.9517e-01, -1.6699e-01,\n",
       "        -3.4266e-01, -2.0659e-01,  1.9835e-02, -8.0646e-01, -2.6090e-01,\n",
       "        -5.2626e-02,  5.6874e-01,  2.5548e-01, -1.5163e-01,  7.6513e-01,\n",
       "        -4.0350e-01,  5.5922e-01, -5.8931e-01,  6.2132e-01,  8.8427e-01,\n",
       "        -3.7487e-01, -1.2693e-01,  5.3712e-02,  1.8846e-01, -3.5682e-01,\n",
       "        -1.2678e-01,  1.2544e-01, -1.2710e-01, -1.7841e-01,  5.1237e-01,\n",
       "        -3.2875e-01,  2.9312e-01, -1.3016e-01, -1.2151e-01, -4.2673e-02,\n",
       "         9.9361e-02, -6.7596e-01,  7.5185e-02,  1.3060e-01,  7.2924e-02,\n",
       "         5.6640e-02,  1.3528e-01,  2.3540e-02,  4.5254e-02,  7.0013e-01,\n",
       "        -2.0324e-01,  3.4277e-01, -1.3742e-01,  7.1266e-02, -2.4637e-01,\n",
       "        -9.3180e-01,  1.2266e-01, -5.0380e-01, -4.3023e-02, -4.5473e-01,\n",
       "        -2.9021e-01, -4.9135e-01, -4.4356e-02, -9.9830e-02, -2.3362e-01,\n",
       "         6.5157e-02, -1.7495e-01, -3.6323e-02, -2.9103e-01,  1.6409e-01,\n",
       "         4.6429e-01,  2.7210e-02,  7.0846e-02,  3.3102e-05,  6.4321e-01,\n",
       "        -1.8138e-01, -2.0020e-01, -4.4424e-01,  2.8831e-01, -1.3371e-03,\n",
       "        -2.3216e-01,  3.0225e-02,  1.7832e-02,  6.8061e-01, -3.0990e-01,\n",
       "        -2.2639e-01, -5.1796e-02,  2.6795e-01, -1.5115e-01, -2.3277e-01,\n",
       "         5.2538e-01, -3.0959e-01,  1.3173e-01, -3.9148e-01,  3.1029e-01,\n",
       "        -1.9799e-01,  1.8313e-01,  3.0448e-03, -6.0153e-02,  3.2217e-01,\n",
       "        -1.0512e-01, -4.0673e-01, -4.1226e-01,  4.6343e-01,  3.6787e-01,\n",
       "        -2.5381e-02, -3.9686e-01, -6.1274e-01,  2.5550e-01,  1.9792e-01,\n",
       "        -2.6103e-02, -2.3546e-01,  8.2955e-02, -4.9217e-02,  1.0231e-02,\n",
       "         1.4127e-01, -5.2163e-01,  2.9780e-01, -1.1389e-02,  2.3434e-02,\n",
       "        -2.2194e-01,  1.8472e-01,  8.4750e-02,  7.9430e-03, -4.1461e-01,\n",
       "         2.2831e-01, -1.7415e-02,  1.4389e-01,  5.8791e-04, -6.0875e-01,\n",
       "         1.8935e-01,  1.1184e-01,  6.0065e-02,  3.3714e-01,  1.1754e-01,\n",
       "         1.9154e-01, -3.3663e-01,  2.1368e-01, -3.9672e-01,  3.6370e-01,\n",
       "         3.2289e-01, -2.8073e-01, -3.6453e-01, -3.3439e-01, -1.6294e-01,\n",
       "         2.1315e-01, -7.1573e-02,  3.0559e-01, -2.1228e-01, -3.9358e-01,\n",
       "         2.1803e-02,  3.4232e-01, -8.2826e-01,  2.5364e-01,  3.2948e-01,\n",
       "        -4.2520e-01,  3.0909e-01,  6.4307e-02, -2.2103e-01,  1.0764e-01,\n",
       "         2.7499e-02,  1.1884e-01, -1.9973e-01, -3.6174e-01, -3.2125e-01,\n",
       "         2.3461e-01,  1.1359e-01,  3.2617e-01, -1.5297e-01, -6.7718e-02,\n",
       "         3.9139e-01,  1.0326e-01,  4.1757e-02, -1.5808e-01, -8.4054e-01,\n",
       "         2.0767e-01, -4.7743e-01,  3.4072e-01,  1.4808e-01,  8.4318e-02,\n",
       "         1.9459e-03,  3.6832e-01, -1.8746e-01,  3.8621e-02, -8.0845e-02,\n",
       "         5.2035e-01,  4.5520e-01,  2.1844e-02,  1.1017e-01, -1.2921e-01,\n",
       "        -3.9236e-01, -3.9673e-01, -6.1090e-02, -3.1415e-01,  1.0945e-01,\n",
       "         1.6663e-01, -2.1793e-01, -2.2178e-01,  4.5724e-02,  2.9892e-02,\n",
       "        -1.0462e-01, -4.2406e-01,  7.0524e-01, -2.8100e-01,  3.2389e-01,\n",
       "        -2.7776e-01, -1.9974e+00,  1.9365e-01,  1.5114e-01,  1.3111e-01,\n",
       "        -4.0156e-01, -2.9107e-01,  2.0276e-02, -3.1984e-01,  1.9169e-01,\n",
       "        -3.1420e-01, -4.2636e-01,  4.0566e-01,  1.1221e-01, -5.0244e-01,\n",
       "         3.1986e-01, -1.7914e-01, -3.8566e-01,  7.8477e-02,  3.7151e-01,\n",
       "         5.3383e-01, -2.7287e-01,  3.1169e-02, -2.7635e-01, -3.8897e-01],\n",
       "       dtype=float32)}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Получение векторов (как из словаря)\n",
    "words = [w for w in tokens if w in model]\n",
    "vectors = {word: model[word] for word in tokens if word in model}\n",
    "vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.45205 , -0.33122 , -0.063607,  0.028325, -0.21372 ,  0.16839 ,\n",
       "       -0.017186,  0.047309, -0.052355, -0.98706 ], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Пример: усечённый вывод вектора (10 первых элементов)\n",
    "vectors[\"love\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Размерность вектора (v(word) ∈ ℝ³⁰⁰)\n",
    "model.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 300)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Размерность векторов (для всего предложения)\n",
    "X = np.vstack([vectors[w] for w in vectors])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i 6.9177027\n",
      "love 6.1360564\n",
      "learning 5.955531\n"
     ]
    }
   ],
   "source": [
    "# Модуль (длина) векторов\n",
    "# Модуль = L2-норма:\n",
    "for w, v in vectors.items():\n",
    "    print(w, np.linalg.norm(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.KeyedVectors at 0x26f1d3b1ba0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalization\n",
    "model.fill_norms()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итоговая схема: \n",
    "\"i\"        → [300 float32] → ‖v‖ ≈ 6.9\n",
    "\"love\"     → [300 float32] → ‖v‖ ≈ 6.1\n",
    "\"learning\" → [300 float32] → ‖v‖ ≈ 6.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maths on text\n",
    "\n",
    "Since the words are embedded into vectors we can now apply mathematical methods on them.\n",
    "\n",
    "### Average vector\n",
    "\n",
    "For example we could build the average vector for a text by using NumPy! This is a straightforward way to build one single representation for a text.\n",
    "\n",
    "- Apply a gensim model on the text\n",
    "- Get all word vectors into a list\n",
    "- Compute and display the average vector of the list\n",
    "- Get it's representation using the gensim most_similar method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'want', 'to', 'be', 'a', 'famous', 'data', 'scientist']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I want to be a famous data scientist\"\n",
    "tokens = text.lower().split()\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 300)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors = {word: model[word] for word in tokens if word in model}\n",
    "X = np.vstack([vectors[w] for w in vectors])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sum_elem=0\\ncnt_elem=0\\nfor w, v in vectors:\\n    sum_elem =+ np.linalg.norm(v)\\n    cnt_elem =+1\\nmean_vector = sum_elem/cnt_elem\\nprint(\"Average_vector: \", mean_vector)\\n\\nmean_vector = np.mean(vectors, axis=0)\\nmean_vector'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The average vector\n",
    "\"\"\"sum_elem=0\n",
    "cnt_elem=0\n",
    "for w, v in vectors:\n",
    "    sum_elem =+ np.linalg.norm(v)\n",
    "    cnt_elem =+1\n",
    "mean_vector = sum_elem/cnt_elem\n",
    "print(\"Average_vector: \", mean_vector)\n",
    "\n",
    "mean_vector = np.mean(vectors, axis=0)\n",
    "mean_vector\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: ['i', 'want', 'to', 'be', 'a', 'famous', 'data', 'scientist']\n",
      "Mean vector shape: (300,)\n",
      "Most similar words:\n",
      "so: 0.783\n",
      "not: 0.772\n",
      "you: 0.767\n",
      "n't: 0.765\n",
      "this: 0.764\n",
      "what: 0.764\n",
      "could: 0.762\n",
      "if: 0.753\n",
      "be: 0.753\n",
      "want: 0.750\n"
     ]
    }
   ],
   "source": [
    "words = [w for w in tokens if w in model]\n",
    "\n",
    "vectors = [model[w] for w in words]\n",
    "mean_vector = np.mean(vectors, axis=0)\n",
    "\n",
    "similar = model.similar_by_vector(mean_vector, topn=10)\n",
    "\n",
    "print(\"Words:\", words)\n",
    "print(\"Mean vector shape:\", mean_vector.shape)\n",
    "print(\"Most similar words:\")\n",
    "for w, s in similar:\n",
    "    print(f\"{w}: {s:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does not work very well in practice, still let us explore further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word similarity\n",
    "\n",
    "We can also compute the similarity between two words by using distance measures (e.g. [cosine similarity](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html), [euclidean distance](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.euclidean_distances.html)...). These measures will calculate the distance between word embeddings in the vector space.\n",
    "\n",
    "Identify what fundamental difference there is between these two metrics when it comes to assessissing similarity between vectors.\n",
    "\n",
    "#### Let's practice!\n",
    "\n",
    "- Compute the cosine and the euclidean distance between those 4 words in a similarity table visualizing it with matplotlib and/or seaborn\n",
    "- Assess which words are the most similar and the most dissimilar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['computer', 'keyboard', 'water', 'ocean'])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [\"computer\",\"keyboard\",\"water\",\"ocean\"]\n",
    "vectors = {w: model[w] for w in words if w in model}\n",
    "vectors.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Косинусное сходство: cos(u,v) = (u⋅v) / (∥u∥⋅∥v∥)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(u, v):\n",
    "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          computer  keyboard  water  ocean\n",
      "computer     1.000     0.441  0.124  0.143\n",
      "keyboard     0.441     1.000  0.054  0.075\n",
      "water        0.124     0.054  1.000  0.477\n",
      "ocean        0.143     0.075  0.477  1.000\n"
     ]
    }
   ],
   "source": [
    "# Матрица косинусного сходства\n",
    "import pandas as pd\n",
    "cosine_matrix = pd.DataFrame(\n",
    "    index=words,\n",
    "    columns=words,\n",
    "    data=[\n",
    "        [cosine_similarity(vectors[w1], vectors[w2]) for w2 in words]\n",
    "        for w1 in words\n",
    "    ]\n",
    ")\n",
    "print(cosine_matrix.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Описание:\n",
    "## диапазон: [-1, 1]\n",
    "## ближе по смыслу → выше значение\n",
    "## Косинус лучше для семантики\n",
    "\n",
    "# Выводы:\n",
    "## computer ↔ keyboard — близкие (техника)\n",
    "## water ↔ ocean — близкие (природа)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Евклидово расстояние:\n",
    "d(u,v)=∥u−v∥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(u, v):\n",
    "    return np.linalg.norm(u - v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          computer  keyboard  water  ocean\n",
      "computer     0.000     7.536  9.546  9.045\n",
      "keyboard     7.536     0.000  9.845  9.316\n",
      "water        9.546     9.845  0.000  7.107\n",
      "ocean        9.045     9.316  7.107  0.000\n"
     ]
    }
   ],
   "source": [
    "# Матрица евклидовых расстояний\n",
    "euclidean_matrix = pd.DataFrame(\n",
    "    index=words,\n",
    "    columns=words,\n",
    "    data=[\n",
    "        [euclidean_distance(vectors[w1], vectors[w2]) for w2 in words]\n",
    "        for w1 in words\n",
    "    ]\n",
    ")\n",
    "print(euclidean_matrix.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Описание:\n",
    "## меньше расстояние → ближе по смыслу\n",
    "## диапазон: [0, +∞)\n",
    "## Евклид чувствителен к масштабу векторов\n",
    "\n",
    "# Выводы:\n",
    "## computer ↔ keyboard — близкие (техника)\n",
    "## water ↔ ocean — близкие (природа)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining things together\n",
    "\n",
    "This next bit of code uses the gensim library to allow you to perform arithmetic operations on vectors. Things you may want to try:\n",
    "\n",
    "Silly additions:\n",
    " - man + hair\n",
    "\n",
    "Checking for some more abstractions:\n",
    " - hair - woman + man\n",
    " - mice - home + city\n",
    " - children - child + goose\n",
    " - paris - france + belgium\n",
    " - triceratops - deer + wolf\n",
    "\n",
    "Bonus points if you can make a function which takes any form of addition and substraction calculations on word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'king' - 'man' + 'woman' = 'queen'\n"
     ]
    }
   ],
   "source": [
    "equals=model.most_similar(positive=['king', 'woman'], negative=['man'])[0][0]\n",
    "print(f\"'king' - 'man' + 'woman' = '{equals}'\")\n",
    "\n",
    "#Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция вычисляет векторное выражение со словами через + и -. Пример: word_vector_expression(\"king - man + woman\", model)\n",
    "# Args:\n",
    "##  expr (str): строка выражения, слова разделены пробелом, используют + и -\n",
    "##  model (KeyedVectors): загруженная модель словарных векторов\n",
    "# Returns: np.ndarray: результирующий вектор\n",
    "def word_vector_expression(expr, model):\n",
    "    tokens = expr.lower().split()\n",
    "    \n",
    "    if not tokens:\n",
    "        raise ValueError(\"Выражение пустое\")\n",
    "    \n",
    "    result_vector = None\n",
    "    current_op = \"+\"  # начальная операция\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token == \"+\" or token == \"-\":\n",
    "            current_op = token\n",
    "        else:\n",
    "            if token not in model:\n",
    "                raise ValueError(f\"Слово '{token}' отсутствует в модели\")\n",
    "            \n",
    "            vec = model[token]\n",
    "            if result_vector is None:\n",
    "                # первый вектор\n",
    "                result_vector = vec.copy()\n",
    "            else:\n",
    "                if current_op == \"+\":\n",
    "                    result_vector += vec\n",
    "                elif current_op == \"-\":\n",
    "                    result_vector -= vec\n",
    "    return result_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.25400138e-02, -1.17890000e-01, -7.89985061e-04, -2.15880007e-01,\n",
       "       -1.63714901e-01,  3.54744017e-01, -3.25149782e-02,  6.18579268e-01,\n",
       "        7.81599998e-01, -1.43030000e+00,  1.69982016e-01,  2.52020001e-01,\n",
       "       -5.42320251e-01,  4.44274992e-01, -1.30129993e-01, -5.55970073e-02,\n",
       "        5.18780053e-01, -4.24700975e-03, -4.23150033e-01,  2.13943005e-01,\n",
       "       -2.04540968e-01,  4.02459979e-01, -7.86240026e-02,  6.78250015e-01,\n",
       "       -3.14300001e-01,  3.14165980e-01, -2.19610333e-02, -5.48509955e-01,\n",
       "       -1.20950937e-02,  4.59619999e-01,  4.30344999e-01,  2.86617011e-01,\n",
       "       -3.48199964e-01,  1.76498994e-01, -7.07549989e-01,  3.28399912e-02,\n",
       "        6.03106022e-01, -2.17429996e-01, -7.45769978e-01, -1.54792011e-01,\n",
       "       -5.42690039e-01, -4.08749968e-01, -1.25423998e-01, -8.10353041e-01,\n",
       "        4.85696018e-01, -3.14729989e-01, -3.00839961e-01, -7.99279988e-01,\n",
       "        4.47989970e-01, -3.70359987e-01, -4.39957976e-01, -4.36357975e-01,\n",
       "       -3.71615022e-01,  5.89354992e-01,  6.44199967e-01,  3.55094016e-01,\n",
       "       -5.39130032e-01,  6.39399812e-02, -2.39185005e-01, -5.86950004e-01,\n",
       "        5.96738994e-01,  1.46560013e-01,  3.55149984e-01,  2.92101979e-01,\n",
       "        3.71299982e-01, -3.30579996e-01, -7.26520061e-01,  4.23150003e-01,\n",
       "        8.68879974e-01, -2.05656990e-01,  1.97739989e-01,  1.00270003e-01,\n",
       "        6.64184988e-01,  3.21026385e-01,  2.44770169e-01, -4.59640026e-01,\n",
       "       -4.76105005e-01,  2.93547045e-02,  3.82780023e-02, -1.05078995e-01,\n",
       "       -2.19496995e-01,  8.73704016e-01,  2.29260013e-01, -7.86899626e-02,\n",
       "        5.11828959e-02,  5.95429957e-01,  7.38408983e-01,  1.36336997e-01,\n",
       "       -3.71364027e-01,  1.77661002e-01, -1.50920004e-01,  4.81968999e-01,\n",
       "       -6.97429925e-02, -1.17154002e-01, -6.26479030e-01,  1.66556999e-01,\n",
       "        1.71494007e-01,  6.95430040e-02,  9.94959950e-01, -3.13650012e-01,\n",
       "        3.96053970e-01, -3.96039993e-01, -7.11849928e-02, -9.75209568e-03,\n",
       "        2.92760134e-02,  2.40460008e-01,  1.98389977e-01,  7.89039969e-01,\n",
       "       -1.30483603e+00,  9.89150032e-02, -1.80038989e-01,  1.50750041e-01,\n",
       "        4.25576977e-02, -4.04849976e-01,  3.63640010e-01, -1.62910014e-01,\n",
       "       -1.00095999e+00,  4.70589966e-01, -5.40479720e-01, -1.17358100e+00,\n",
       "       -6.19329989e-01, -7.31152952e-01, -3.51900011e-02,  1.18653983e-01,\n",
       "        3.30034971e-01, -1.82039924e-02, -7.60647058e-01,  6.50566280e-01,\n",
       "        2.53750086e-02, -5.89150012e-01,  3.59299004e-01, -2.28400156e-03,\n",
       "       -5.63030005e-01, -2.18835995e-01, -2.82490015e-01,  2.59261012e-01,\n",
       "        4.51609977e-02, -5.70527971e-01,  8.77770185e-02, -3.07268977e-01,\n",
       "        3.88460010e-01,  6.52120113e-02,  4.83099997e-01, -3.67690027e-01,\n",
       "        1.48070037e-01,  6.83660030e-01, -4.70600128e-02,  4.44869995e-01,\n",
       "       -1.99520022e-01, -6.96609974e-01,  1.08449996e-01,  5.88649988e-01,\n",
       "       -9.66350079e-01, -6.42420053e-01,  3.56029004e-01, -1.11117005e+00,\n",
       "        8.23917985e-01, -7.30239987e-01,  6.16119981e-01,  8.17100048e-01,\n",
       "        2.83280015e-01, -5.84689975e-01, -6.41919911e-01,  1.02640003e-01,\n",
       "       -2.29199991e-01,  2.41866589e-01,  1.74216002e-01,  7.41239965e-01,\n",
       "        2.99459994e-01, -5.19209981e-01,  4.86674011e-01,  3.48572016e-01,\n",
       "       -3.10220003e-01, -5.38699985e-01,  3.82983983e-01, -2.84709513e-01,\n",
       "        4.28056002e-01,  3.63290995e-01,  4.18708980e-01, -8.97149980e-01,\n",
       "        3.08135986e-01, -4.07615006e-01,  1.88170001e-01,  6.11335516e-01,\n",
       "       -9.32970047e-01, -5.60999513e-02,  9.76639926e-01, -2.84426004e-01,\n",
       "        1.56511307e-01,  6.31345987e-01, -1.29521981e-01,  3.08449984e-01,\n",
       "        8.49001110e-03,  1.01761997e+00,  2.15199977e-01, -8.04200053e-01,\n",
       "       -4.33920026e-01, -1.70540035e-01,  2.00150013e-01,  1.06929988e-01,\n",
       "        1.41799998e+00,  1.10426806e-01,  1.17561007e+00,  2.83556014e-01,\n",
       "       -3.42536002e-01,  3.49409938e-01, -6.08299971e-02,  4.64949965e-01,\n",
       "       -1.07613993e+00, -9.62390006e-01, -7.24669695e-02,  2.09004015e-01,\n",
       "       -4.00759995e-01,  2.58082986e-01, -7.37680197e-02,  1.08678007e+00,\n",
       "        4.26440001e-01, -7.90014863e-04,  4.56153005e-01, -6.90389991e-01,\n",
       "       -4.76299822e-02, -3.47629994e-01,  3.23439986e-01,  1.43815011e-01,\n",
       "        4.57067996e-01, -6.42230034e-01, -2.25639984e-01,  3.54210019e-01,\n",
       "       -2.98539996e-01,  3.15180004e-01,  8.10310006e-01,  2.89950997e-01,\n",
       "        2.29306996e-01, -2.80653983e-01,  3.53738785e-01, -6.88206017e-01,\n",
       "        2.84897000e-01, -2.35789999e-01, -6.63980007e-01,  4.09379989e-01,\n",
       "        6.79530025e-01,  3.51184994e-01, -4.36192989e-01,  4.22011018e-01,\n",
       "       -6.33291006e-01, -5.34796000e-01, -2.08460242e-02, -2.88522005e-01,\n",
       "       -9.69410092e-02, -4.45406020e-01,  6.31439984e-01, -9.30040032e-02,\n",
       "        7.49469042e-01, -6.56558990e-01,  9.47489917e-01,  4.55780029e-01,\n",
       "        5.29527128e-01,  2.62960047e-01, -7.75171995e-01, -4.34529990e-01,\n",
       "        7.97950029e-01, -2.93733984e-01,  1.58316001e-01,  3.72044027e-01,\n",
       "       -3.56279969e-01,  6.37780055e-02, -2.57700384e-02,  1.46535993e-01,\n",
       "       -4.32260007e-01,  7.28639960e-01, -2.31999993e-01,  5.79500049e-02,\n",
       "       -5.22130013e-01,  2.90939987e-01, -3.93669963e-01,  1.97062984e-01,\n",
       "       -1.27259994e+00, -2.11243004e-01, -8.73572350e-01, -4.23629016e-01,\n",
       "       -4.90680993e-01,  6.02051020e-01,  5.19110024e-01,  8.40876937e-01,\n",
       "        5.24519980e-01,  8.22729945e-01, -2.16439962e-02, -3.12640995e-01,\n",
       "       -2.64890015e-01,  9.01804447e-01, -8.70099962e-02,  2.26426005e-01,\n",
       "       -8.13548028e-01,  2.17738032e-01,  1.37065008e-01, -1.02860019e-01,\n",
       "        6.16129994e-01,  1.51829988e-01,  4.31059897e-02,  1.19540036e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Проверка:\n",
    "expr_1 = \"hair - woman + man\"\n",
    "expr_2 = \"mice - home + city\"\n",
    "expr_3 = \"children - child + goose\"\n",
    "expr_4 = \"paris - france + belgium\"\n",
    "expr_5 = \"triceratops - deer + wolf\"\n",
    "\n",
    "vec_1 = word_vector_expression(expr_1, model)\n",
    "vec_2 = word_vector_expression(expr_1, model)\n",
    "vec_3 = word_vector_expression(expr_1, model)\n",
    "vec_4 = word_vector_expression(expr_1, model)\n",
    "vec_5 = word_vector_expression(expr_1, model)\n",
    "\n",
    "vec_3\n",
    "# vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('king', 0.8065858483314514), ('queen', 0.689616322517395), ('monarch', 0.5575491189956665), ('throne', 0.5565375089645386), ('princess', 0.5518684387207031)]\n"
     ]
    }
   ],
   "source": [
    "# Посмотрим ближайшие слова\n",
    "similar = model.similar_by_vector(vec, topn=5)\n",
    "print(similar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you play with these examples (or others). You quickly notice both the powerful levels of abstraction and the gaping limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More resources\n",
    "* [Why do we use word embeddings in NLP?](https://towardsdatascience.com/why-do-we-use-embeddings-in-nlp-2f20e1b632d2)\n",
    "* [More details on what word embeddings are exactly?](https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
